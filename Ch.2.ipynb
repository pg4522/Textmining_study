{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e0b549",
   "metadata": {},
   "source": [
    "# 1. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a9964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20652bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mkw/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package webtext to /Users/mkw/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/mkw/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /Users/mkw/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mkw/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ecd2c",
   "metadata": {},
   "source": [
    "## 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0676f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(sent_tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541aefd",
   "metadata": {},
   "source": [
    "## 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9cd4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78de3ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a18e1",
   "metadata": {},
   "source": [
    "## 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a00d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e1db1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[abc]', 'How are you, boy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02197d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '5', '9']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[0123456789]', '3a7b5c9d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356a17b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[\\w]', \"3a 7b_ '.^&5c9d\")\n",
    "# [\\w] -> [a-zA-Z0-9_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e70c085d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'are', 'you', 'boy']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[\\w]+', 'How are you, boy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "621ebc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'I', \"can't\", 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "020a002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"can't\", 'there']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Sorry, I can't go there.\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")\n",
    "\n",
    "print(tokenizer.tokenize(text1.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfd758",
   "metadata": {},
   "source": [
    "## 노이즈와 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69a4b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower())\n",
    "\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75131957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you're\", 'haven', 'it', 'about', 'just', \"hadn't\", 'and', 'his', 'is', \"shan't\", 'into', 'll', 'yours', 'over', 'of', 'off', \"didn't\", \"won't\", 'him', 'out', 'further', 'ain', 'theirs', 'why', 'isn', 'where', 'than', 'few', 'above', 'can', 'or', 'to', \"it's\", 'an', 'hasn', \"you'll\", 'that', \"mightn't\", 'you', 'same', 'from', \"don't\", 'should', \"she's\", \"you've\", \"isn't\", 'again', 'only', 'itself', 'this', 'hers', 'will', \"you'd\", 'he', 'her', 'other', \"shouldn't\", 'before', 'didn', 'if', 'do', 'be', 'me', \"doesn't\", 'shouldn', 'aren', 'more', 'weren', 'at', 'what', 'she', 'between', 'd', 'needn', 'wouldn', \"mustn't\", 'did', 'the', 'during', 'who', \"hasn't\", 'does', 'its', 'your', 'then', 'a', 'some', 'these', \"aren't\", 'our', 'now', 'herself', 'don', 'their', 'y', 'ma', 'under', 'down', 'mustn', \"weren't\", 'after', \"needn't\", 'hadn', 'most', 'as', 'has', 'wasn', 'yourself', 'being', 'in', 'doesn', 'doing', 'each', 'ourselves', 't', 'won', 'very', 'when', 'nor', 'while', 'mightn', 'those', 'once', 's', 'they', 'we', 'o', \"couldn't\", 'too', 'until', 'shan', \"wasn't\", 'himself', 've', 'here', 'no', \"wouldn't\", 'there', 'myself', 'was', 'i', 'are', 'up', 'such', 'own', 'how', \"that'll\", 'with', 'themselves', 'them', 'had', 'both', 'm', 're', 'couldn', 'been', 'am', 'ours', 'any', 'on', 'through', 'but', 'were', 'all', 'have', \"haven't\", 'by', 'against', 'my', 'whom', 'having', 'so', \"should've\", 'not', 'which', 'below', 'yourselves', 'because', 'for'}\n"
     ]
    }
   ],
   "source": [
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463e2d7",
   "metadata": {},
   "source": [
    "# 2. 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3708d",
   "metadata": {},
   "source": [
    "## 어간 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8cde7",
   "metadata": {},
   "source": [
    "- 포터 스테머\n",
    "    - 모든 단어가 같은 규칙에 따라 변환\n",
    "    - 영어 분야에서 사실상의 표준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d92179aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc4325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para)\n",
    "print(tokens)\n",
    "\n",
    "result = [stemmer.stem(token) for token in tokens]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7a3d0",
   "metadata": {},
   "source": [
    "- 랭카스터 스테머"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b74fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07f83f",
   "metadata": {},
   "source": [
    "## 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5dcb2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/mkw/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54a93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookery\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80435781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming result:  believ\n",
      "lemmatizing result:  belief\n",
      "lemmatizing result:  believe\n"
     ]
    }
   ],
   "source": [
    "#lemmatizing과 stemming 비교\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print('stemming result: ', stemmer.stem('believes'))\n",
    "print('lemmatizing result: ', lemmatizer.lemmatize('believes'))\n",
    "print('lemmatizing result: ', lemmatizer.lemmatize('believes', pos = 'v'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
