{"cells":[{"cell_type":"markdown","id":"ed572c51","metadata":{"id":"ed572c51"},"source":["# 1. Language Model"]},{"cell_type":"markdown","id":"6132c873","metadata":{"id":"6132c873"},"source":["- 더 자연스러운 문장에 더 높은 확률을 부여한다.\n","- GPT (Generative Pre-trained Transformer)\n","    - 주로 자연어 문장 생성에 특화된 모델\n","    - 트랜스포머의 디코더 부분만 따로 떼어서 학습 모형으로 사용함.\n","    - 인코더에서 디코더로의 어텐션은 생략하고, 셀프 어텐션은 순방향만 적용됨.\n","- 비지도 학습이 가능하다.\n","- Fine Tuning (미세조정학습)을 통해 지도학습에 활용이 가능하다."]},{"cell_type":"markdown","id":"74f6024c","metadata":{"id":"74f6024c"},"source":["# 2. BERT"]},{"cell_type":"markdown","id":"9b2f0060","metadata":{"id":"9b2f0060"},"source":["- Bidirectional Encoder Representations from Transformers\n","- 트랜스포머에서 인코더 부분만 사용한 모형, 양방향 셀프 어텐션을 구현하고 있다.\n","- 트랜스포머의 인코더 부분을 떼어서 사용함."]},{"cell_type":"markdown","id":"db9c073c","metadata":{"id":"db9c073c"},"source":["# 3. Pre-training & Fine-Tuning"]},{"cell_type":"markdown","id":"fed1a6f2","metadata":{"id":"fed1a6f2"},"source":["## 1) Pre-training"]},{"cell_type":"markdown","id":"f391f27b","metadata":{"id":"f391f27b"},"source":["- 언어에 대한 이해를 높이기 위한 비지도학습\n","- 'masking', 즉 단어를 가리고, 가린 단어를 예측하게 한다.\n","- Masked language model: 순서와 관계없이 문장 안에서 랜덤한 위치의 단어를 지우고, 모형이 이 단어들을 예측하도록 한다.\n","- 가려진 단어는 문장의 중간에 위치하며, 양쪽에 단어들이 있어 양방향 셀프 어텐션을 모두 이용해 예측하는 것이 가능하다.\n","- BERT의 경우, 원래 문장에서 약 15%의 단어들을 마스킹한다."]},{"cell_type":"markdown","id":"1e6f5e4b","metadata":{"id":"1e6f5e4b"},"source":["- 두 개의 문장을 다룰 수 있도록 한다.\n","- 두 문장을 구분하는 토큰을 정의하고 두 문장 사이에 넣어서 하나의 시퀀스를 만든 후에 인코더에서 한 번에 처리하도록 한다."]},{"cell_type":"markdown","id":"77ced0fe","metadata":{"id":"77ced0fe"},"source":["## 2) Fine-Tuning"]},{"cell_type":"markdown","id":"3430c1f8","metadata":{"id":"3430c1f8"},"source":["- 실제 수행하고자 하는 작업에 대한 지도학습 (ex: 문서 분류)"]},{"cell_type":"markdown","id":"8536c7e0","metadata":{"id":"8536c7e0"},"source":["# 4. 사전학습된 BERT"]},{"cell_type":"code","execution_count":1,"id":"bfc2e342","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfc2e342","executionInfo":{"status":"ok","timestamp":1661913713093,"user_tz":-540,"elapsed":11205,"user":{"displayName":"K Wani","userId":"15155454679480857830"}},"outputId":"25fd599f-8678-40e8-dc01-ee9bde89cb54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 15.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 70.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 69.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"]}],"source":["pip install transformers"]},{"cell_type":"markdown","id":"fea32254","metadata":{"id":"fea32254"},"source":["- Pipeline\n","    - 목적에 맞게 적절한 토크나이저와 BERT 모형 객체를 선언한다.\n","    - 토크나이저를 이용해 주어진 텍스트를 BERT 모형에 맞는 입력으로 변환한다.\n","    - 변환된 입력을 모형에 전달하고 결과와 확률 등을 받아 전달한다.\n","    - 토큰 수가 많아지면 (보통 512개) 에러가 발생한다. -> 자동 클래스 사용"]},{"cell_type":"markdown","id":"396ab0b9","metadata":{"id":"396ab0b9"},"source":["- 작업 종류\n","    - sentiment-analysis : 감성분석\n","    - text-classification : 문서분류\n","    - question-answering : 질의응답\n","    - text-generation : 문서생성\n","    - translation : 기계번역\n","    - summarization : 문서요약"]},{"cell_type":"code","execution_count":2,"id":"b16be311","metadata":{"id":"b16be311","executionInfo":{"status":"ok","timestamp":1661913728443,"user_tz":-540,"elapsed":8706,"user":{"displayName":"K Wani","userId":"15155454679480857830"}}},"outputs":[],"source":["from transformers import pipeline"]},{"cell_type":"markdown","source":["## 감성분석"],"metadata":{"id":"h-qH5cxY03cK"},"id":"h-qH5cxY03cK"},{"cell_type":"code","execution_count":11,"id":"7d2a0c6e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7d2a0c6e","executionInfo":{"status":"ok","timestamp":1661914503447,"user_tz":-540,"elapsed":6371,"user":{"displayName":"K Wani","userId":"15155454679480857830"}},"outputId":"d9b4c602-9140-41b7-fc9c-eb9192139696"},"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"stream","name":"stdout","text":["#감성분석 결과: POSITIVE, 감성스코어: 0.9999\n"]}],"source":["clf = pipeline('sentiment-analysis')\n","result = clf('what a beautiful day!')[0]\n","print('#감성분석 결과: %s, 감성스코어: %0.4f' %(result['label'], result['score']))"]},{"cell_type":"markdown","source":["## 문서 생성"],"metadata":{"id":"HspS9rsF00AV"},"id":"HspS9rsF00AV"},{"cell_type":"code","source":["text_generator = pipeline(\"text-generation\")\n","result = text_generator(\"Alice was beginning to get very tired of sitting by her sister on the bank,\")\n","print(result[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o9w8fQHfxw1V","executionInfo":{"status":"ok","timestamp":1661914515217,"user_tz":-540,"elapsed":8646,"user":{"displayName":"K Wani","userId":"15155454679480857830"}},"outputId":"2fef9cbf-3828-4e1d-a8c1-7f152014c477"},"id":"o9w8fQHfxw1V","execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Alice was beginning to get very tired of sitting by her sister on the bank, and as she got really carried away, her father had to rush her into the carriage, and by making it harder he would have prevented any more of their doing it.\"\n"]}]},{"cell_type":"markdown","source":["# 5. 자동 클래스"],"metadata":{"id":"x6rvVTiU08Dg"},"id":"x6rvVTiU08Dg"},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch"],"metadata":{"id":"ypCgQznLyhsO","executionInfo":{"status":"ok","timestamp":1661914692055,"user_tz":-540,"elapsed":361,"user":{"displayName":"K Wani","userId":"15155454679480857830"}}},"id":"ypCgQznLyhsO","execution_count":13,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('bert-base-cased-finetuned-mrpc')\n","model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased-finetuned-mrpc')\n","\n","input_sentence = 'She angered me with her inappropriate comments, rumor-spreading, and disrespectfulness at the formal dinner table'\n","target_sequence = 'She made me angry when she was rude at dinner'\n","\n","tokens = tokenizer(input_sentence, target_sequence, return_tensors = 'pt')\n","\n","logits = model(**tokens).logits\n","\n","results = torch.softmax(logits, dim = 1).tolist()[0]\n","\n","for i, label in enumerate(['no', 'yes']):\n","    print(f'{label}: {int(round(results[i]*100))}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CW4wxYj01bUf","executionInfo":{"status":"ok","timestamp":1661915009029,"user_tz":-540,"elapsed":5989,"user":{"displayName":"K Wani","userId":"15155454679480857830"}},"outputId":"d6423c78-9cd1-4c02-a603-c9efa26b65b1"},"id":"CW4wxYj01bUf","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["no: 29%\n","yes: 71%\n"]}]},{"cell_type":"code","source":["target_sequence = \"The boy quickly ran across the finish line, seizing yet another victory\"\n","tokens = tokenizer(input_sentence, target_sequence, return_tensors=\"pt\")\n","logits = model(**tokens).logits\n","results = torch.softmax(logits, dim=1).tolist()[0]\n","\n","for i, label in enumerate(['no', 'yes']):\n","    print(f\"{label}: {int(round(results[i] * 100))}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWgMMICq2YEC","executionInfo":{"status":"ok","timestamp":1661915137320,"user_tz":-540,"elapsed":522,"user":{"displayName":"K Wani","userId":"15155454679480857830"}},"outputId":"c818ea27-9663-4bc2-e382-bf99b0eeaaac"},"id":"qWgMMICq2YEC","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["no: 95%\n","yes: 5%\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import movie_reviews\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","nltk.download('movie_reviews')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0LfnzWO03O-V","executionInfo":{"status":"ok","timestamp":1661915209112,"user_tz":-540,"elapsed":965,"user":{"displayName":"K Wani","userId":"15155454679480857830"}},"outputId":"2f281694-2cf4-44ce-f97a-0a233bb96de9"},"id":"0LfnzWO03O-V","execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["fileids = movie_reviews.fileids()\n","reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n","categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] \n","\n","# label을 0, 1의 값으로 변환\n","label_dict = {'pos':1, 'neg':0}\n","y = np.array([label_dict[c] for c in categories])\n","\n","X_train, X_test, y_train, y_test = train_test_split(reviews, y, test_size=0.2, random_state=7)\n","print('Train set count: ', len(X_train))\n","print('Test set count: ', len(X_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OcvT70fj3ZfH","executionInfo":{"status":"ok","timestamp":1661915275960,"user_tz":-540,"elapsed":509,"user":{"displayName":"K Wani","userId":"15155454679480857830"}},"outputId":"396875f6-047c-4058-f94c-ef4df6adc77a"},"id":"OcvT70fj3ZfH","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set count:  1600\n","Test set count:  400\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import torch.nn.functional as F\n","\n","# cuda를 이용한 GPU 연산이 가능하면 cuda를 사용하고, 아니면 cpu를 사용\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# Auto Classes를 이용해 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","# 모델을 gpu로 옮겨서 연산을 준비\n","model = model.to(device)\n","\n","batch_size = 10 # 모형으로 한번에 예측할 데이터의 수\n","y_pred = [] # 전체 예측결과를 저장\n","\n","num_batch = len(y_test)//batch_size\n","\n","for i in range(num_batch):\n","    inputs = tokenizer(X_test[i*batch_size:(i+1)*batch_size], truncation=True, padding=True, return_tensors=\"pt\")\n","    # 토큰화 결과를 GPU로 이동\n","    inputs = inputs.to(device)\n","    # 모형으로 결과를 예측\n","    logits = model(**inputs).logits\n","    # 결과값을 클래스에 대한 확률로 변환\n","    pred = F.softmax(logits, dim=-1)\n","    # 예측결과를 CPU로 가져와서 넘파이로 변환한 후, argmax로 확률이 가장 큰 클래스를 선택함\n","    results = pred.cpu().detach().numpy().argmax(axis=1)\n","    \n","    # 전체 예측결과에 추가\n","    y_pred.extend(results.tolist())\n","\n","# gpu 메모리를 비움\n","torch.cuda.empty_cache()\n","\n","score = sum(y_test == np.array(y_pred))/len(y_test)\n","print(\"NLTK 영화리뷰 감성분석 정확도:\", score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IItmOZ6S31of","executionInfo":{"status":"ok","timestamp":1661915854743,"user_tz":-540,"elapsed":341497,"user":{"displayName":"K Wani","userId":"15155454679480857830"}},"outputId":"1ba8ae18-5bf0-42ad-fc83-1e25e2f9a78e"},"id":"IItmOZ6S31of","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["NLTK 영화리뷰 감성분석 정확도: 0.8425\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Ch.14.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}